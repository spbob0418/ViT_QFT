import torch
import re

def rename_keys(checkpoint, is_transfer_learning):
    """ ì²´í¬í¬ì¸íŠ¸ì˜ í‚¤ë¥¼ ëª¨ë¸ í‚¤ì™€ ë§ì¶°ì„œ ë³€í™˜ """
    if is_transfer_learning: 
        for k in ['classifier.weight', 'classifier.bias']:
            if k in checkpoint:
                del checkpoint[k]

    key_mapping = {}
    qkv_weights = {}
    qkv_biases = {}

    print("ğŸ” Key Mapping Start...")

    for i, old_key in enumerate(checkpoint.keys()):  # ë£¨í”„ ì¹´ìš´íŠ¸ ì¶”ê°€
        
        new_key = old_key

        # Embeddings ë³€í™˜
        new_key = new_key.replace("vit.embeddings.cls_token", "cls_token")
        new_key = new_key.replace("vit.embeddings.position_embeddings", "pos_embed")
        new_key = new_key.replace("vit.embeddings.patch_embeddings.projection.weight", "patch_embed.proj.weight")
        new_key = new_key.replace("vit.embeddings.patch_embeddings.projection.bias", "patch_embed.proj.bias")
        
        # Transformer Blocks ë³€í™˜
        match = re.match(r'vit\.encoder\.layer\.(\d+)\.(.*)', new_key)
        if match:
            layer_num, sub_key = match.groups()
            layer_num = int(layer_num)

            # qkvë¥¼ í•©ì³ì•¼ í•˜ë¯€ë¡œ query, key, valueë¥¼ ë³„ë„ë¡œ ì €ì¥
            if "attention.attention.query.weight" in sub_key:
                qkv_weights.setdefault(layer_num, {})["query"] = checkpoint[old_key]
                continue
            if "attention.attention.key.weight" in sub_key:
                qkv_weights.setdefault(layer_num, {})["key"] = checkpoint[old_key]
                continue
            if "attention.attention.value.weight" in sub_key:
                qkv_weights.setdefault(layer_num, {})["value"] = checkpoint[old_key]
                continue

            if "attention.attention.query.bias" in sub_key:
                qkv_biases.setdefault(layer_num, {})["query"] = checkpoint[old_key]
                continue
            if "attention.attention.key.bias" in sub_key:
                qkv_biases.setdefault(layer_num, {})["key"] = checkpoint[old_key]
                continue
            if "attention.attention.value.bias" in sub_key:
                qkv_biases.setdefault(layer_num, {})["value"] = checkpoint[old_key]
                continue

            sub_key = sub_key.replace("attention.output.dense", "attn.proj")
            sub_key = sub_key.replace("intermediate.dense", "mlp.fc1")
            sub_key = sub_key.replace("output.dense", "mlp.fc2")
            sub_key = sub_key.replace("layernorm_before", "norm1")
            sub_key = sub_key.replace("layernorm_after", "norm2")
            new_key = f"blocks.modules_list.{layer_num}.{sub_key}"

        # LayerNorm ë³€í™˜
        new_key = new_key.replace("vit.layernorm.weight", "norm.weight")
        new_key = new_key.replace("vit.layernorm.bias", "norm.bias")

        if not is_transfer_learning: 
            print("head ë„˜ê¸°ê¸°")
            new_key = new_key.replace("classifier.weight", "head.weight")
            new_key = new_key.replace("classifier.bias", "head.bias")

        key_mapping[old_key] = new_key  

    return key_mapping, qkv_weights, qkv_biases
